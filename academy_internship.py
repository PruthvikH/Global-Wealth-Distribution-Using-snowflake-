# -*- coding: utf-8 -*-
"""academy_Internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JKdiSNQIvk99lvkvyP3sSbRTemS4wTA_

Medallion arch
"""

import pandas as pd

# Load the raw data (Bronze Layer)
file_path = "/content/average-net-worth-by-country-2024.csv"  # Replace with your actual file path
bronze_df = pd.read_csv(file_path)

# Step 1: Remove null values
bronze_df_cleaned = bronze_df.dropna()

# Step 2: Remove duplicate rows (if any)
bronze_df_cleaned = bronze_df_cleaned.drop_duplicates()

# Save the cleaned data to a new Bronze layer file
bronze_layer_path = "/content/medallion_arch/bronze.csv"  # Replace with your desired path
bronze_df_cleaned.to_csv(bronze_layer_path, index=False)

print(f"Bronze layer processing complete. Cleaned data saved to {bronze_layer_path}.")

"""Upload t S3"""

!pip install boto3

import boto3

# Initialize S3 client
s3_client = boto3.client(
    's3',
    aws_access_key_id='',
    aws_secret_access_key='',
    region_name='ap-south-1'  # Example: 'us-east-1'
)

# Variables
bucket_name = "internship.academy"
file_name = "/content/medallion_arch/bronze.csv"       # Replace with the file path you want to upload
s3_key = "bronze.csv"          # Replace with the name in S3

# Upload file (without try)
s3_client.upload_file(file_name, bucket_name, s3_key)

print(f"File {file_name} uploaded to {bucket_name}/{s3_key}.")

import boto3

# Initialize S3 client
s3_client = boto3.client(
    's3',
    aws_access_key_id='',
    aws_secret_access_key='',
    region_name='ap-south-1'  # Example: 'us-east-1'
)

# Variables
bucket_name = "internship.academy"
file_name = "/content/countries_with_continents_cleaned.csv"       # Replace with the file path you want to upload
s3_key = "countries_with_continents_cleaned.csv"          # Replace with the name in S3

# Upload file (without try)
s3_client.upload_file(file_name, bucket_name, s3_key)

print(f"File {file_name} uploaded to {bucket_name}/{s3_key}.")